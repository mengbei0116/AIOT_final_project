{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-18T20:35:41.435118Z",
     "iopub.status.busy": "2024-05-18T20:35:41.43408Z",
     "iopub.status.idle": "2024-05-18T20:35:41.441512Z",
     "shell.execute_reply": "2024-05-18T20:35:41.440531Z",
     "shell.execute_reply.started": "2024-05-18T20:35:41.435072Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class\n",
    "\n",
    "We define a custom `WasteDataset` class that inherits from PyTorch's `Dataset` class. This class is responsible for loading and preprocessing the images from the dataset.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "The `__init__` method takes the following parameters:\n",
    "- `root_dir`: The root directory containing the dataset images.\n",
    "- `split`: The dataset split (train, validation, or test).\n",
    "- `transform`: Optional image transformations to be applied.\n",
    "\n",
    "Inside the `__init__` method, we:\n",
    "1. Store the `root_dir`, `transform`, and `split` parameters.\n",
    "2. Get the list of class names by listing the directories in `root_dir`.\n",
    "3. Initialize empty lists for `image_paths` and `labels`.\n",
    "4. Iterate over each class directory and its subfolders ('default' and 'real_world').\n",
    "5. Shuffle the image names in each subfolder.\n",
    "6. Based on the `split` parameter, select a portion of the images (60% for train, 20% for validation, 20% for test).\n",
    "7. Append the image paths and corresponding labels to the respective lists.\n",
    "\n",
    "### Length and Item Retrieval\n",
    "\n",
    "The `__len__` method returns the total number of images in the dataset.\n",
    "\n",
    "The `__getitem__` method takes an `index` and returns the image and its corresponding label at that index. It:\n",
    "1. Retrieves the image path and label using the provided index.\n",
    "2. Opens the image using `Image.open()` and converts it to RGB format.\n",
    "3. Applies the specified image transformations, if any.\n",
    "4. Returns the transformed image and its label.\n",
    "\n",
    "This custom dataset class allows us to easily load and preprocess the waste images for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T20:14:30.879689Z",
     "iopub.status.busy": "2024-05-18T20:14:30.878875Z",
     "iopub.status.idle": "2024-05-18T20:14:30.891855Z",
     "shell.execute_reply": "2024-05-18T20:14:30.89078Z",
     "shell.execute_reply.started": "2024-05-18T20:14:30.87964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the dataset class (modified to include a split parameter)\n",
    "class WasteDataset(Dataset):\n",
    "    def __init__(self, root_dir, split, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = ['aluminum_can','paper_meal_box','paper_cup','plastic_bottle'] #sorted(os.listdir(root_dir)) \n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for i, class_name in enumerate(self.classes):\n",
    "                class_dir = os.path.join(root_dir, class_name)\n",
    "                image_names = os.listdir(class_dir)\n",
    "                random.shuffle(image_names)\n",
    "                \n",
    "                if split == 'train':\n",
    "                    image_names = image_names[:int(0.6 * len(image_names))]\n",
    "                elif split == 'val':\n",
    "                    image_names = image_names[int(0.6 * len(image_names)):int(0.8 * len(image_names))]\n",
    "                else:  # split == 'test'\n",
    "                    image_names = image_names[int(0.8 * len(image_names)):]\n",
    "                \n",
    "                for image_name in image_names:\n",
    "                    self.image_paths.append(os.path.join(class_dir, image_name))\n",
    "                    self.labels.append(i)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        label = self.labels[index]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Path and Hyperparameters\n",
    "\n",
    "We set the following dataset path and hyperparameters:\n",
    "- `dataset_path`: The path to the directory containing the dataset images.\n",
    "- `batch_size`: The number of samples per batch during training and evaluation.\n",
    "- `num_epochs`: The number of epochs to train the model.\n",
    "- `learning_rate`: The learning rate for the optimizer.\n",
    "\n",
    "These hyperparameters can be adjusted based on the specific requirements and available computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T20:14:32.037681Z",
     "iopub.status.busy": "2024-05-18T20:14:32.037281Z",
     "iopub.status.idle": "2024-05-18T20:14:32.04286Z",
     "shell.execute_reply": "2024-05-18T20:14:32.041702Z",
     "shell.execute_reply.started": "2024-05-18T20:14:32.037649Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set the dataset path and hyperparameters\n",
    "dataset_path = 'dataset'\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Loaders\n",
    "\n",
    "We define a composition of image transformations using `transforms.Compose`:\n",
    "1. `transforms.Resize((224, 224))`: Resizes the images to a fixed size of (224, 224) pixels.\n",
    "2. `transforms.ToTensor()`: Converts the images to PyTorch tensors.\n",
    "3. `transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])`: Normalizes the image tensors using the specified mean and standard deviation values.\n",
    "\n",
    "These transformations ensure that the images are preprocessed consistently before being fed into the model.\n",
    "\n",
    "We create instances of the `WasteDataset` class for the train, validation, and test splits, passing the `dataset_path`, `split`, and `transform` parameters. This allows us to load the dataset images with the specified transformations for each split.\n",
    "\n",
    "Finally, we create data loaders for each dataset using `DataLoader`:\n",
    "- `train_dataloader`: Loads the training data in batches of size `batch_size` and shuffles the samples.\n",
    "- `val_dataloader`: Loads the validation data in batches of size `batch_size` without shuffling.\n",
    "- `test_dataloader`: Loads the test data in batches of size `batch_size` without shuffling.\n",
    "\n",
    "The data loaders provide an efficient way to iterate over the dataset during training and evaluation, handling batching and shuffling as specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from rembg import remove\n",
    "\n",
    "def rotate_and_pad_dynamic(image, angle, background_color=(0, 0, 0), min_size=400):\n",
    "    # Step 1: 旋轉並展開\n",
    "    # image = remove(image)\n",
    "    rotated = image.rotate(angle, expand=True, fillcolor=background_color)\n",
    "\n",
    "    # Step 2: 動態計算正方形背景尺寸（取最大邊長，與 min_size 比較）\n",
    "    side = max(min_size, rotated.width, rotated.height)\n",
    "    \n",
    "    # Step 3: 建立正方形背景並貼上圖片\n",
    "    background = Image.new(\"RGB\", (side, side), background_color)\n",
    "    paste_x = (side - rotated.width) // 2\n",
    "    paste_y = (side - rotated.height) // 2\n",
    "    background.paste(rotated, (paste_x, paste_y))\n",
    "\n",
    "    return background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T20:14:32.624823Z",
     "iopub.status.busy": "2024-05-18T20:14:32.624099Z",
     "iopub.status.idle": "2024-05-18T20:14:32.803503Z",
     "shell.execute_reply": "2024-05-18T20:14:32.802748Z",
     "shell.execute_reply.started": "2024-05-18T20:14:32.624786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create the datasets and data loaders\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Lambda(lambda img: rotate_and_pad_dynamic(img, angle=random.randint(-30, 30))),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "train_dataset = WasteDataset(dataset_path, split='train', transform=transform)\n",
    "val_dataset = WasteDataset(dataset_path, split='val', transform=transform)\n",
    "test_dataset = WasteDataset(dataset_path, split='test', transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T20:14:33.554196Z",
     "iopub.status.busy": "2024-05-18T20:14:33.553502Z",
     "iopub.status.idle": "2024-05-18T20:14:34.680924Z",
     "shell.execute_reply": "2024-05-18T20:14:34.679759Z",
     "shell.execute_reply.started": "2024-05-18T20:14:33.554163Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\awfan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\awfan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models, datasets, transforms\n",
    "import torchvision\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# Create the model, loss function, and optimizer\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# 凍結所有參數（如要 fine-tune 可改為 False）\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to('cuda')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T20:14:34.683937Z",
     "iopub.status.busy": "2024-05-18T20:14:34.68296Z",
     "iopub.status.idle": "2024-05-18T20:20:55.152721Z",
     "shell.execute_reply": "2024-05-18T20:20:55.151731Z",
     "shell.execute_reply.started": "2024-05-18T20:14:34.683892Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 1.3142, Val Loss: 0.9340\n",
      "Epoch [2/5], Train Loss: 0.8577, Val Loss: 0.6346\n",
      "Epoch [3/5], Train Loss: 0.6445, Val Loss: 0.5277\n",
      "Epoch [4/5], Train Loss: 0.5270, Val Loss: 0.4136\n",
      "Epoch [5/5], Train Loss: 0.4832, Val Loss: 0.3719\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Lists to store the training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_dataloader:\n",
    "        images = images.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    train_loss /= len(train_dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images = images.to('cuda')\n",
    "            labels = labels.to('cuda')\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    val_loss /= len(val_dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'waste_classification_model_resnet_18.pth')\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 載入並測試模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awfan\\AppData\\Local\\Temp\\ipykernel_35088\\63598644.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  testmodel = torch.load('waste_classification_model_resnet_18.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 93.19%\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "testmodel = torch.load('waste_classification_model_resnet_18.pth') \n",
    "testmodel.eval()  # 切換到評估模式\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # 評估時不計算梯度\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = testmodel(images)             # 前向推論，shape=(batch_size, num_classes)\n",
    "        _, preds = torch.max(outputs, 1)    # 取每列最大值的索引作為預測\n",
    "\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision import transforms\n",
    "\n",
    "# 建立反標準化的 transform（ImageNet mean/std）\n",
    "unnormalize = transforms.Normalize(\n",
    "    mean=[-m/s for m, s in zip([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])],\n",
    "    std=[1/s for s in [0.229, 0.224, 0.225]]\n",
    ")\n",
    "\n",
    "# 儲存處理過的圖片\n",
    "def save_tensor_as_image(tensor, filename):\n",
    "    img = unnormalize(tensor.squeeze(0)).clamp(0, 1)  # 去 batch 維 + 限定 0~1\n",
    "    img_pil = to_pil_image(img)\n",
    "    os.makedirs('rotated_image', exist_ok=True)\n",
    "    img_pil.save(os.path.join('rotated_image', filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0200, 0.8412, 0.0855, 0.0533]], device='cuda:0')"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awfan\\AppData\\Local\\Temp\\ipykernel_35088\\2049077889.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  testmodel = torch.load('waste_classification_model_resnet_18.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image2.png → 預測為: 紙餐盒\n",
      "tensor([[0.0771, 0.0047, 0.9025, 0.0158]], device='cuda:0')\n",
      "image3.png → 預測為: 紙杯\n",
      "tensor([[0.0114, 0.0012, 0.0019, 0.9855]], device='cuda:0')\n",
      "image4.png → 預測為: 寶特瓶\n",
      "tensor([[0.7436, 0.0157, 0.1261, 0.1147]], device='cuda:0')\n",
      "image5.png → 預測為: 鐵鋁罐\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from rembg import remove\n",
    "\n",
    "# 設定測試圖片資料夾\n",
    "test_image_dir = 'custom_test/'\n",
    "\n",
    "# 模型輸入所需的轉換（依照你訓練時使用的 transform）\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Lambda(lambda img: rotate_and_pad_dynamic(img, angle=random.randint(-30, 30))),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 載入你訓練好的模型\n",
    "testmodel = torch.load('waste_classification_model_resnet_18.pth')\n",
    "testmodel.eval()\n",
    "testmodel.to('cuda')\n",
    "# 類別標籤（順序需與訓練時相符）\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "# 預測每一張圖片\n",
    "for filename in os.listdir(test_image_dir):\n",
    "    if filename.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "        image_path = os.path.join(test_image_dir, filename)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        #image = remove(image).convert('RGB')\n",
    "        \n",
    "        input_tensor = transform(image).unsqueeze(0)  # 增加 batch dimension\n",
    "        input_tensor = input_tensor.to('cuda')\n",
    "        save_tensor_as_image(input_tensor, filename)  # 儲存處理過的圖片\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = testmodel(input_tensor)\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "            print(probabilities)\n",
    "            predicted_class = class_names[output.argmax(1).item()]\n",
    "        \n",
    "        if predicted_class.startswith('aluminum_') or predicted_class.startswith('steel'):\n",
    "            print(f\"{filename} → 預測為: 鐵鋁罐\")\n",
    "        elif predicted_class.startswith('paper_meal'):\n",
    "            print(f\"{filename} → 預測為: 紙餐盒\")\n",
    "        elif predicted_class.startswith('plastic'):\n",
    "            print(f\"{filename} → 預測為: 寶特瓶\")\n",
    "        elif predicted_class.startswith('paper_c'):\n",
    "            print(f\"{filename} → 預測為: 紙杯\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5037535,
     "sourceId": 8452655,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
