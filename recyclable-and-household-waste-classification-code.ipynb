{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-18T20:35:41.435118Z",
     "iopub.status.busy": "2024-05-18T20:35:41.43408Z",
     "iopub.status.idle": "2024-05-18T20:35:41.441512Z",
     "shell.execute_reply": "2024-05-18T20:35:41.440531Z",
     "shell.execute_reply.started": "2024-05-18T20:35:41.435072Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class\n",
    "\n",
    "We define a custom `WasteDataset` class that inherits from PyTorch's `Dataset` class. This class is responsible for loading and preprocessing the images from the dataset.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "The `__init__` method takes the following parameters:\n",
    "- `root_dir`: The root directory containing the dataset images.\n",
    "- `split`: The dataset split (train, validation, or test).\n",
    "- `transform`: Optional image transformations to be applied.\n",
    "\n",
    "Inside the `__init__` method, we:\n",
    "1. Store the `root_dir`, `transform`, and `split` parameters.\n",
    "2. Get the list of class names by listing the directories in `root_dir`.\n",
    "3. Initialize empty lists for `image_paths` and `labels`.\n",
    "4. Iterate over each class directory and its subfolders ('default' and 'real_world').\n",
    "5. Shuffle the image names in each subfolder.\n",
    "6. Based on the `split` parameter, select a portion of the images (60% for train, 20% for validation, 20% for test).\n",
    "7. Append the image paths and corresponding labels to the respective lists.\n",
    "\n",
    "### Length and Item Retrieval\n",
    "\n",
    "The `__len__` method returns the total number of images in the dataset.\n",
    "\n",
    "The `__getitem__` method takes an `index` and returns the image and its corresponding label at that index. It:\n",
    "1. Retrieves the image path and label using the provided index.\n",
    "2. Opens the image using `Image.open()` and converts it to RGB format.\n",
    "3. Applies the specified image transformations, if any.\n",
    "4. Returns the transformed image and its label.\n",
    "\n",
    "This custom dataset class allows us to easily load and preprocess the waste images for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T20:14:30.879689Z",
     "iopub.status.busy": "2024-05-18T20:14:30.878875Z",
     "iopub.status.idle": "2024-05-18T20:14:30.891855Z",
     "shell.execute_reply": "2024-05-18T20:14:30.89078Z",
     "shell.execute_reply.started": "2024-05-18T20:14:30.87964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the dataset class (modified to include a split parameter)\n",
    "class WasteDataset(Dataset):\n",
    "    def __init__(self, root_dir, split, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for i, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            image_names = os.listdir(class_dir)\n",
    "            random.shuffle(image_names)\n",
    "            \n",
    "            if split == 'train':\n",
    "                image_names = image_names[:int(0.6 * len(image_names))]\n",
    "            elif split == 'val':\n",
    "                image_names = image_names[int(0.6 * len(image_names)):int(0.8 * len(image_names))]\n",
    "            else:  # split == 'test'\n",
    "                image_names = image_names[int(0.8 * len(image_names)):]\n",
    "            \n",
    "            for image_name in image_names:\n",
    "                self.image_paths.append(os.path.join(class_dir, image_name))\n",
    "                self.labels.append(i)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        label = self.labels[index]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model Architecture\n",
    "\n",
    "We define a convolutional neural network (CNN) model called `CNN` that inherits from PyTorch's `nn.Module` class. This model architecture consists of convolutional layers, pooling layers, and fully connected layers.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "The `__init__` method takes the following parameter:\n",
    "- `num_classes`: The number of output classes in the classification task.\n",
    "\n",
    "Inside the `__init__` method, we define the layers of the CNN:\n",
    "1. `conv1`: A 2D convolutional layer with 3 input channels, 32 output channels, a kernel size of 3, stride of 1, and padding of 1.\n",
    "2. `relu`: A ReLU activation function.\n",
    "3. `maxpool`: A 2D max pooling layer with a kernel size of 2 and stride of 2.\n",
    "4. `conv2`: Another 2D convolutional layer with 32 input channels, 64 output channels, a kernel size of 3, stride of 1, and padding of 1.\n",
    "5. `fc1`: A fully connected layer that takes the flattened output of `conv2` and maps it to 512 features.\n",
    "6. `fc2`: The final fully connected layer that takes the 512 features and maps them to the number of output classes.\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "The `forward` method defines the forward pass of the CNN model. It takes an input tensor `x` and applies the following operations:\n",
    "1. Pass `x` through `conv1`, followed by `relu` activation and `maxpool`.\n",
    "2. Pass the output through `conv2`, followed by `relu` activation and `maxpool`.\n",
    "3. Flatten the output of `conv2` using `x.view(x.size(0), -1)`.\n",
    "4. Pass the flattened tensor through `fc1`, followed by `relu` activation.\n",
    "5. Pass the output of `fc1` through `fc2` to obtain the final output.\n",
    "\n",
    "The output of the `forward` method represents the predicted class scores for each input sample.\n",
    "\n",
    "This CNN architecture is designed to learn hierarchical features from the input images and make predictions based on those features. The convolutional layers capture local patterns, the pooling layers reduce spatial dimensions, and the fully connected layers perform the final classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T20:14:31.519571Z",
     "iopub.status.busy": "2024-05-18T20:14:31.518633Z",
     "iopub.status.idle": "2024-05-18T20:14:31.528915Z",
     "shell.execute_reply": "2024-05-18T20:14:31.527796Z",
     "shell.execute_reply.started": "2024-05-18T20:14:31.519533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))  # 新增自適應池化層\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)  # 修改 fc1 的輸入大小\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.adaptive_pool(x)  # 應用自適應池化層\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Path and Hyperparameters\n",
    "\n",
    "We set the following dataset path and hyperparameters:\n",
    "- `dataset_path`: The path to the directory containing the dataset images.\n",
    "- `batch_size`: The number of samples per batch during training and evaluation.\n",
    "- `num_epochs`: The number of epochs to train the model.\n",
    "- `learning_rate`: The learning rate for the optimizer.\n",
    "\n",
    "These hyperparameters can be adjusted based on the specific requirements and available computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T20:14:32.037681Z",
     "iopub.status.busy": "2024-05-18T20:14:32.037281Z",
     "iopub.status.idle": "2024-05-18T20:14:32.04286Z",
     "shell.execute_reply": "2024-05-18T20:14:32.041702Z",
     "shell.execute_reply.started": "2024-05-18T20:14:32.037649Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set the dataset path and hyperparameters\n",
    "dataset_path = 'clean_dataset'\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Loaders\n",
    "\n",
    "We define a composition of image transformations using `transforms.Compose`:\n",
    "1. `transforms.Resize((224, 224))`: Resizes the images to a fixed size of (224, 224) pixels.\n",
    "2. `transforms.ToTensor()`: Converts the images to PyTorch tensors.\n",
    "3. `transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])`: Normalizes the image tensors using the specified mean and standard deviation values.\n",
    "\n",
    "These transformations ensure that the images are preprocessed consistently before being fed into the model.\n",
    "\n",
    "We create instances of the `WasteDataset` class for the train, validation, and test splits, passing the `dataset_path`, `split`, and `transform` parameters. This allows us to load the dataset images with the specified transformations for each split.\n",
    "\n",
    "Finally, we create data loaders for each dataset using `DataLoader`:\n",
    "- `train_dataloader`: Loads the training data in batches of size `batch_size` and shuffles the samples.\n",
    "- `val_dataloader`: Loads the validation data in batches of size `batch_size` without shuffling.\n",
    "- `test_dataloader`: Loads the test data in batches of size `batch_size` without shuffling.\n",
    "\n",
    "The data loaders provide an efficient way to iterate over the dataset during training and evaluation, handling batching and shuffling as specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from rembg import remove\n",
    "\n",
    "def rotate_and_pad_dynamic(image, angle, background_color=(0, 0, 0), min_size=400):\n",
    "    # Step 1: 旋轉並展開\n",
    "    # image = remove(image)\n",
    "    rotated = image.rotate(angle, expand=True, fillcolor=background_color)\n",
    "\n",
    "    # Step 2: 動態計算正方形背景尺寸（取最大邊長，與 min_size 比較）\n",
    "    side = max(min_size, rotated.width, rotated.height)\n",
    "    \n",
    "    # Step 3: 建立正方形背景並貼上圖片\n",
    "    background = Image.new(\"RGB\", (side, side), background_color)\n",
    "    paste_x = (side - rotated.width) // 2\n",
    "    paste_y = (side - rotated.height) // 2\n",
    "    background.paste(rotated, (paste_x, paste_y))\n",
    "\n",
    "    return background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T20:14:32.624823Z",
     "iopub.status.busy": "2024-05-18T20:14:32.624099Z",
     "iopub.status.idle": "2024-05-18T20:14:32.803503Z",
     "shell.execute_reply": "2024-05-18T20:14:32.802748Z",
     "shell.execute_reply.started": "2024-05-18T20:14:32.624786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create the datasets and data loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: rotate_and_pad_dynamic(img, angle=random.randint(-30, 30))),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = WasteDataset(dataset_path, split='train', transform=transform)\n",
    "val_dataset = WasteDataset(dataset_path, split='val', transform=transform)\n",
    "test_dataset = WasteDataset(dataset_path, split='test', transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T20:14:33.554196Z",
     "iopub.status.busy": "2024-05-18T20:14:33.553502Z",
     "iopub.status.idle": "2024-05-18T20:14:34.680924Z",
     "shell.execute_reply": "2024-05-18T20:14:34.679759Z",
     "shell.execute_reply.started": "2024-05-18T20:14:33.554163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create the model, loss function, and optimizer\n",
    "num_classes = len(train_dataset.classes)\n",
    "model = CNN(num_classes).to('cuda')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.001):\n",
    "        self.patience = patience  # 忍耐次數\n",
    "        self.delta = delta        # 最小改善\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T20:14:34.683937Z",
     "iopub.status.busy": "2024-05-18T20:14:34.68296Z",
     "iopub.status.idle": "2024-05-18T20:20:55.152721Z",
     "shell.execute_reply": "2024-05-18T20:20:55.151731Z",
     "shell.execute_reply.started": "2024-05-18T20:14:34.683892Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Train Loss: 1.5567, Val Loss: 1.4177\n",
      "Epoch [2/30], Train Loss: 1.3789, Val Loss: 1.2304\n",
      "Epoch [3/30], Train Loss: 1.2848, Val Loss: 1.1979\n",
      "Epoch [4/30], Train Loss: 1.2205, Val Loss: 1.1336\n",
      "Epoch [5/30], Train Loss: 1.1179, Val Loss: 1.1582\n",
      "Epoch [6/30], Train Loss: 1.1108, Val Loss: 1.0891\n",
      "Epoch [7/30], Train Loss: 1.0584, Val Loss: 1.0583\n",
      "Epoch [8/30], Train Loss: 1.0481, Val Loss: 1.0806\n",
      "Epoch [9/30], Train Loss: 1.0341, Val Loss: 0.9942\n",
      "Epoch [10/30], Train Loss: 1.0065, Val Loss: 1.0017\n",
      "Epoch [11/30], Train Loss: 0.9616, Val Loss: 0.9788\n",
      "Epoch [12/30], Train Loss: 0.9293, Val Loss: 0.9828\n",
      "Epoch [13/30], Train Loss: 0.9043, Val Loss: 0.9331\n",
      "Epoch [14/30], Train Loss: 0.9038, Val Loss: 0.9337\n",
      "Epoch [15/30], Train Loss: 0.8819, Val Loss: 0.9163\n",
      "Epoch [16/30], Train Loss: 0.8626, Val Loss: 0.9959\n",
      "Epoch [17/30], Train Loss: 0.8359, Val Loss: 1.0101\n",
      "Epoch [18/30], Train Loss: 0.7970, Val Loss: 0.9578\n",
      "Epoch [19/30], Train Loss: 0.7942, Val Loss: 0.8420\n",
      "Epoch [20/30], Train Loss: 0.7642, Val Loss: 0.8809\n",
      "Epoch [21/30], Train Loss: 0.7987, Val Loss: 0.8653\n",
      "Epoch [22/30], Train Loss: 0.7486, Val Loss: 0.8662\n",
      "Epoch [23/30], Train Loss: 0.6935, Val Loss: 0.8350\n",
      "Epoch [24/30], Train Loss: 0.6940, Val Loss: 0.8153\n",
      "Epoch [25/30], Train Loss: 0.6814, Val Loss: 0.8013\n",
      "Epoch [26/30], Train Loss: 0.6423, Val Loss: 0.8259\n",
      "Epoch [27/30], Train Loss: 0.6309, Val Loss: 0.8349\n"
     ]
    }
   ],
   "source": [
    "# Lists to store the training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "early_stopper = EarlyStopping(patience=5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_dataloader:\n",
    "        images = images.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    train_loss /= len(train_dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images = images.to('cuda')\n",
    "            labels = labels.to('cuda')\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    val_loss /= len(val_dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    early_stopper(val_loss)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'waste_classification_model.pth')\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 載入並測試模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "model.eval()  # 切換到評估模式\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # 評估時不計算梯度\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)             # 前向推論，shape=(batch_size, num_classes)\n",
    "        _, preds = torch.max(outputs, 1)    # 取每列最大值的索引作為預測\n",
    "\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awfan\\AppData\\Local\\Temp\\ipykernel_23932\\2875735909.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  testmodel = torch.load('waste_classification_model.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2.png → 預測為: 紙杯\n",
      "image3.png → 預測為: 紙餐盒\n",
      "image4.png → 預測為: 紙杯\n",
      "image5.png → 預測為: 紙杯\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from rembg import remove\n",
    "\n",
    "# 設定測試圖片資料夾\n",
    "test_image_dir = 'custom_test/'\n",
    "\n",
    "# 模型輸入所需的轉換（依照你訓練時使用的 transform）\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Lambda(lambda img: rotate_and_pad_dynamic(img, angle=random.randint(-10, 10))),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 載入你訓練好的模型\n",
    "testmodel = torch.load('waste_classification_model.pth')\n",
    "testmodel.eval()\n",
    "testmodel.to('cuda')\n",
    "# 類別標籤（順序需與訓練時相符）\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "# 預測每一張圖片\n",
    "for filename in os.listdir(test_image_dir):\n",
    "    if filename.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "        image_path = os.path.join(test_image_dir, filename)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = remove(image).convert('RGB')\n",
    "        input_tensor = transform(image).unsqueeze(0)  # 增加 batch dimension\n",
    "        input_tensor = input_tensor.to('cuda')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = testmodel(input_tensor)\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "            predicted_class = class_names[output.argmax(1).item()]\n",
    "\n",
    "        if predicted_class.startswith('aluminum_'):\n",
    "            print(f\"{filename} → 預測為: 鐵鋁罐\")\n",
    "        elif predicted_class.startswith('paper_meal'):\n",
    "            print(f\"{filename} → 預測為: 紙餐盒\")\n",
    "        elif predicted_class.startswith('plastic'):\n",
    "            print(f\"{filename} → 預測為: 寶特瓶\")\n",
    "        elif predicted_class.startswith('paper_c'):\n",
    "            print(f\"{filename} → 預測為: 紙杯\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5037535,
     "sourceId": 8452655,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
